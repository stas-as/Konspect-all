{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import PIL\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import pathlib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3670\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pathlib.Path(\"data/flower_photos\")\n",
    "batch_size = 32\n",
    "img_wigth= 180\n",
    "img_hieght= 180\n",
    "image_count = len(list(df.glob(\"*/*.jpg\")))\n",
    "print(image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n",
      "clas names ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "train_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    df,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed= 123,\n",
    "    image_size=(img_wigth,img_hieght),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "valid_df = tf.keras.utils.image_dataset_from_directory(\n",
    "    df,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed= 123,\n",
    "    image_size=(img_wigth,img_hieght),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = train_df.class_names\n",
    "print(f\"clas names {class_names}\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_df.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "valid_ds = valid_df.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
